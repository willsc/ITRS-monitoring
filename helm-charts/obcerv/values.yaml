# The version of Obcerv to install
# (required)
version: "1.3.2"

defaultImagePullPolicy: "IfNotPresent"

defaultStorageClass: ""

# Image tag used for all Platform images
platformImageTag: "1.3.2"

# (required)
apps:
  # Fully-qualified hostname that clients use to connect to Obcerv UI applications
  # (required)
  externalHostname: ""

  ingress:
    annotations: {}

    tlsSecret: ""

# Docker registry
docker:
  # URL of the Docker registry containing the Obcerv images
  registry: "docker.itrsgroup.com"

  # Name of the secret containing the Docker registry credentials
  secret: "itrsdocker"

# Global TLS
tls:
  # TLS settings for traffic inside the Kubernetes cluster
  internal:
    # Whether TLS is enabled for traffic inside the Kubernetes cluster
    enabled: true

  # TLS settings for traffic from outside the Kubernetes cluster
  external:
    # Whether to enable the creation of self-signed TLS certificates for each Ingress
    selfSigned: false

# Service mesh
mesh:
  # Resources for the mesh proxy on high-throughput workloads
  resourcesHot:
    config.linkerd.io/proxy-cpu-request: "0.4"
    config.linkerd.io/proxy-cpu-limit: "1"
    config.linkerd.io/proxy-memory-request: "512Mi"
    config.linkerd.io/proxy-memory-limit: "512Mi"

  # Resources for the mesh proxy on non high-throughput workloads
  resourcesStandard:
    config.linkerd.io/proxy-cpu-request: "0.2"
    config.linkerd.io/proxy-cpu-limit: "0.2"
    config.linkerd.io/proxy-memory-request: "256Mi"
    config.linkerd.io/proxy-memory-limit: "256Mi"

# Zookeeper workload
zookeeper:
  # (min: 1, immutable)
  replicas: 1

  image: "obcerv/zookeeper"

  jvmHeap: "400M"

  # Initial size of the PVC. This value is used when the PVC is created - if it needs to be expanded
  # later, modify the PVC directly instead of changing this parameter.
  # (min: 1Gi, immutable)
  diskSize: "1Gi"

  # (immutable)
  storageClass: ""

  resources:
    requests:
      memory: "512Mi"
      cpu: "100m"
    limits:
      memory: "512Mi"
      cpu: "100m"

  nodeSelector: {}

  # Adjust the Zookeeper workload's logging level. Acceptable values are: OFF, FATAL, ERROR, WARN,
  # INFO, DEBUG, TRACE, ALL.
  logLevel: "INFO"

# Kafka workload
kafka:
  image: "obcerv/kafka"

  # required as a default here to support a 1.3.2 upgrade job - can be removed in a later version if desired
  clientPort: 9092

  # Initial size of the PVC. This value is used when the PVC is created - if it needs to be expanded
  # later, modify the PVC directly instead of changing this parameter.
  # (min: 1Gi, immutable)
  diskSize: "10Gi"

  # (immutable)
  storageClass: ""

  # (min: 1, immutable)
  replicas: 1

  heapOpts: "-XX:+UseContainerSupport -XX:InitialRAMPercentage=75 -XX:MaxRAMPercentage=75"

  # (min: 1048576, max: 2147483647)
  defaultChangelogTopicSegmentBytes: 268435456

  # (min: 1048576, max: 2147483647)
  defaultLogSegmentBytes: 536870912

  # (min: 1048576, max: 67108864)
  defaultMessageMaxBytes: 10485772

  # Number of partitions for topics that don't have an explicit partition setting. Note this only
  # applies to topics that don't exist yet - the number of partitions in existing topics will not be
  # changed.
  # (min: 1, max: 100)
  defaultPartitions: 6

  # (min: 60000, max: 2592000000)
  defaultRetentionMillis: 21600000

  # Kafka consumer properties
  consumer:
    # (min: 0, max: 5000)
    fetchMaxWaitMs: 500

    # (min: 0, max: 33554432)
    fetchMinBytes: 1048576

  # Kafka producer properties
  producer:
    # (min: 0, max: 33554432)
    batchSize: 1048576

    # (min: 0, max: 134217728)
    bufferMemory: 67108864

    # (min: 0, max: 5000)
    lingerMs: 100

    # (min: 0, max: 33554432)
    maxRequestSize: 2097152

  resources:
    requests:
      memory: "1Gi"
      cpu: "300m"
    limits:
      memory: "3Gi"
      cpu: "500m"

  nodeSelector: {}

  tolerations: []

  # Adjust the Kafka workload's logging level. Acceptable values are: OFF, FATAL, ERROR, WARN, INFO,
  # DEBUG, TRACE, ALL.
  logLevel: "INFO"

# Downsampled Metrics Stream workload
downsampledMetricsStream:
  image: "obcerv/downsampled-metrics-stream"

  # (immutable)
  storageClass: ""

  # Initial size of the PVC used for raw data. This value is used when the PVC is created - if it
  # needs to be expanded later, modify the PVC directly instead of changing this parameter.
  # (min: 1Gi, immutable)
  diskSize: "5Gi"

  # Initial size of the PVC used for bucketed data. This value is used when the PVC is created - if it
  # needs to be expanded later, modify the PVC directly instead of changing this parameter.
  # (min: 1Gi, immutable)
  bucketedDiskSize: "5Gi"

  maxPollRecords: 5000

  # (min: 1)
  replicas: 1

  # (min: 1)
  bucketedReplicas: 1

  # (min: 1)
  threads: 1

  # (min: 1)
  bucketedThreads: 1

  rawRocksdb:
    # (min: 8388608)
    totalOffHeapMemory: 8388608

    # (min: 0, max: 0.5)
    indexFilterRatio: 0.25

    # (min: 4194304)
    totalMemTableMemory: 6291456

    # (min: 1024)
    blockSize: 4096

    # (min: 1048576)
    writeBufferSize: 3145728

  bucketedRocksdb:
    # (min: 8388608)
    totalOffHeapMemory: 8388608

    # (min: 0, max: 0.5)
    indexFilterRatio: 0.25

    # (min: 4194304)
    totalMemTableMemory: 6291456

    # (min: 1024)
    blockSize: 4096

    # (min: 1048576)
    writeBufferSize: 3145728

  storeRetention:
    # (min: 1)
    metrics5m: 10800000

    # (min: 1)
    metrics15m: 21600000

    # (min: 1)
    metrics1h: 86400000

    # (min: 1)
    metrics3h: 86400000

    # (min: 1)
    metrics12h: 259200000

    # (min: 1)
    metrics1d: 259200000

    # (min: 1)
    statuses: 432000000

  resources:
    requests:
      memory: "1Gi"
      cpu: "250m"
    limits:
      memory: "1Gi"
      cpu: "500m"

  bucketedResources:
    requests:
      memory: "1Gi"
      cpu: "250m"
    limits:
      memory: "1Gi"
      cpu: "500m"

  nodeSelector: {}

  # Adjust the downsampled metrics stream workload's logging level. Acceptable values are: OFF, ERROR,
  # WARN, INFO, DEBUG, TRACE, ALL.
  logLevel: "INFO"

# Entity Stream workloads
entityStream:
  final:
    # (min: 1000)
    entitySerdeCacheSize: 10000

    # (min: 0)
    enrichmentRulesetBatchTimeoutMillis: 5000

    # (min: 0)
    containmentRulesBatchTimeoutMillis: 5000

    # (min: 600)
    entityStatisticsPeriodicitySeconds: 3600

    image: "obcerv/final-entity-stream"

    # The jvmOpts should only be set when explicitly wanting to override the default JVM heap settings
    # as  described in the corresponding deployment manifest. For example, if the container memory
    # limits (RAM) is 10 GB, then the default JVM heap size would be 40% of that, namely 4GB, which is
    # presumably too much.  By configuring the jvmOpts to e.g., -Xms128M -Xmx228M,  these attributes
    # would precedence over the default settings.
    jvmOpts: ""

    resources:
      requests:
        memory: "768Mi"
        cpu: "150m"
      limits:
        memory: "768Mi"
        cpu: "300m"

    rocksdb:
      # The portion of the cache reserved for high priority entries. When combined with the
      # 'writeBufferRatio' the sum of both must be no more than 1.
      # (min: 0.01, max: 0.99)
      indexAndFilterRatio: 0.2

      # Total (in mebibytes) of native memory RocksDB cache will use
      # (min: 1)
      memoryMib: 100

      # Total (in mebibytes) of individual write buffers. This value of (writeBufferRatio * memoryMib)
      # must be greater than this value.
      # (min: 1)
      writeBufferMib: 8

      # The total portion of the cache reserved for write buffers. When combined with the
      # 'indexAndFilterRatio' the sum of both must be no more than 1.
      # (min: 0.01, max: 0.99)
      writeBufferRatio: 0.5

      enableStats: false

    # Adjust the final entity stream workload's logging level. Acceptable values are: OFF, ERROR,
    # WARN, INFO, DEBUG, TRACE and ALL.
    logLevel: "INFO"

  intermediate:
    image: "obcerv/intermediate-entity-stream"

    # The jvmOpts should only be set when explicitly wanting to override the default JVM heap settings
    # as  described in the corresponding deployment manifest. For example, if the container memory
    # limits (RAM) is 10 GB, then the default JVM heap size would be 40% of that, namely 4GB, which is
    # presumably too much.  By configuring the jvmOpts to e.g., -Xms128M -Xmx228M,  these attributes
    # would precedence over the default settings.
    jvmOpts: ""

    # (min: 1)
    replicas: 1

    resources:
      requests:
        memory: "768Mi"
        cpu: "300m"
      limits:
        memory: "768Mi"
        cpu: "500m"

    rocksdb:
      # The portion of the cache reserved for high priority entries. When combined with the
      # 'writeBufferRatio' the sum of both must be no more than 1.
      # (min: 0.01, max: 0.99)
      indexAndFilterRatio: 0.2

      # Total (in mebibytes) of native memory RocksDB cache will use
      # (min: 1)
      memoryMib: 100

      # Total (in mebibytes) of individual write buffers. This value of (writeBufferRatio * memoryMib)
      # must be greater than this value.
      # (min: 1)
      writeBufferMib: 8

      # The total portion of the cache reserved for write buffers. When combined with the
      # 'indexAndFilterRatio' the sum of both must be no more than 1.
      # (min: 0.01, max: 0.99)
      writeBufferRatio: 0.5

      enableStats: false

    # Adjust the intermediate entity stream workload's logging level. Acceptable values are: OFF,
    # ERROR, WARN, INFO, DEBUG, TRACE, ALL.
    logLevel: "INFO"

# Signals Stream workload
signalsStream:
  image: "obcerv/signals-stream"

  # (min: 1)
  replicas: 1

  resources:
    requests:
      memory: "256Mi"
      cpu: "150m"
    limits:
      memory: "512Mi"
      cpu: "300m"

  rocksdb:
    # The portion of the cache reserved for high priority entries. When combined with the
    # 'writeBufferRatio' the sum of both must be no more than 1.
    # (min: 0.01, max: 0.99)
    indexAndFilterRatio: 0.2

    # Total (in mebibytes) of native memory RocksDB cache will use
    # (min: 1)
    memoryMib: 100

    # Total (in mebibytes) of individual write buffers. This value of (writeBufferRatio * memoryMib)
    # must be greater than this value.
    # (min: 1)
    writeBufferMib: 8

    # The total portion of the cache reserved for write buffers. When combined with the
    # 'indexAndFilterRatio' the sum of both must be no more than 1.
    # (min: 0.01, max: 0.99)
    writeBufferRatio: 0.5

    enableStats: false

  # Adjust the signals stream workload's logging level. Acceptable values are: OFF, ERROR, WARN, INFO,
  # DEBUG, TRACE, ALL.
  logLevel: "INFO"

# Timescale workload
timescale:
  # StorageClass of the PVC used for non-timeseries table and index data.
  # (immutable)
  dataStorageClass: ""

  # Initial size of the PVC used for non-timeseries and index data. This disk will store all data by
  # default unless the configuration parameter 'timeseriesDiskCount' is > 0. In that case, the default
  # data disk size can be reduced significantly as all timeseries data will be stored on different
  # disks. Total disk size can be calculated with the following function: dataDiskSize +
  # (timeseriesDiskSize * timeseriesDiskCount). This value is used when the PVC is created - if it
  # needs to be expanded later, modify the PVC directly instead of changing this parameter.
  # (min: 1Gi, immutable)
  dataDiskSize: "10Gi"

  # Storage class for timeseries data.
  # (immutable)
  timeseriesStorageClass: ""

  # Number of timeseries data disks to create to store all timeseries data on separate disks from
  # non-timeseries data. This is useful when constrained by max disk sizes, and/or expanding storage.
  # When this configuration parameter is > 0, the data is spread out amongst these disks instead of
  # all the data being stored on the default disk. This parameter can only be increased and not
  # lowered.
  # (min: 0)
  timeseriesDiskCount: 0

  # Initial size of each PVC used for timeseries data. Total disk size can be calculated with the
  # following function: dataDiskSize + (timeseriesDiskSize * timeseriesDiskCount). This value is used
  # when the PVC is created - if it needs to be expanded later, modify the PVC directly instead of
  # changing this parameter.
  # (min: 1Gi, immutable)
  timeseriesDiskSize: "5Gi"

  # Number of nodes in the cluster (the primary plus all secondaries)
  # (min: 1)
  clusterSize: 1

  image: "obcerv/timescale"

  # (min: 100)
  maxConnections: 100

  nodeSelector: {}

  pgExporterImage: "obcerv/postgres-exporter"

  pljavaJvmOptions: "-Xshare:auto -Xms8m -Xmx8m -Xss196k -XX:MaxMetaspaceSize=16m -XX:ReservedCodeCacheSize=4m -XX:+UseSerialGC"

  resources:
    requests:
      memory: "512Mi"
      cpu: "200m"
    limits:
      memory: "1Gi"
      cpu: "300m"

  # Time after a single timescale table chunk is compressed. This value is applied for all tables that
  # have compression enabled.
  compressAfter: "3h"

  retention:
    entity_attributes:
      chunkSize: "2d"

      retention: "2y"

    metrics:
      chunkSize: "3h"

      retention: "60d"

    metrics_5m:
      chunkSize: "12h"

      retention: "180d"

    metrics_15m:
      chunkSize: "36h"

      retention: "180d"

    metrics_1h:
      chunkSize: "2d"

      retention: "1y"

    metrics_3h:
      chunkSize: "5d"

      retention: "1y"

    metrics_12h:
      chunkSize: "7d"

      retention: "2y"

    metrics_1d:
      chunkSize: "7d"

      retention: "2y"

    statuses:
      chunkSize: "7d"

      retention: "2y"

  # (min: 25, max: 40)
  sharedBuffersPercentage: 25

  tolerations: []

  # Initial size of the PVC used for WAL replication data. This value is used when the PVC is created
  # - if it needs to be expanded later, modify the PVC directly instead of changing this parameter.
  # Must be set to at least 1Gi.
  # (min: 1Gi, immutable)
  walDiskSize: "5Gi"

  # StorageClass of the PVC used for WAL replication data.
  # (immutable)
  walStorageClass: ""

# Loki workload
loki:
  # Initial size of the PVC. This value is used when the PVC is created - if it needs to be expanded
  # later, modify the PVC directly instead of changing this parameter.
  # (min: 1Gi, immutable)
  diskSize: "1Gi"

  image: "obcerv/loki"

  # (min: 1, max: 1024)
  ingestionBurstSize: 24

  # (min: 1, max: 1024)
  ingestionRateLimit: 16

  # (min: 1024, max: 1073741824)
  maxPayloadSize: 16777216

  # Max number of streams per user, set to zero for no max
  # (min: 0, max: 1000000)
  maxStreamsPerUser: 100000

  nodeSelector: {}

  # (min: 1, max: 1, immutable)
  replicas: 1

  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "512Mi"
      cpu: "300m"

  retentionTime: "168h"

  # (immutable)
  storageClass: ""

  # Adjust the Loki workload's logging level. Acceptable values are: debug, info, warn, error.
  logLevel: "info"

# Etcd workload
etcd:
  autoCompactionRetention: 1000

  # Initial size of the PVC. This value is used when the PVC is created - if it needs to be expanded
  # later, modify the PVC directly instead of changing this parameter.
  # (min: 1Gi, immutable)
  diskSize: "1Gi"

  image: "obcerv/etcd"

  # Adjust the etcd workload's logging level. Acceptable values are: debug, info, warn, error, panic,
  # fatal.
  logLevel: "info"

  nodeSelector: {}

  # The size of the storage quota, i.e. the maximum amount of data permitted in the database before a
  # `NOSPACE` alarm is raised and etcd becomes unavailable.  8GB is a suggested maximum size for
  # normal environments.
  # (min: 536870912)
  quotaBackendBytes: 2147483648

  # (min: 1)
  replicas: 1

  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "768Mi"
      cpu: "300m"

  # Configures the number of applied Raft entries to hold in-memory before compaction.  The value is a
  # tradeoff between higher memory usage and better availabilities of slow followers.
  # (min: 1000, max: 100000)
  snapshotCount: 10000

  # (immutable)
  storageClass: ""

  tolerations: []

# Ingestion Service workload
# (required)
ingestion:
  # Fully-qualified hostname that clients use to connect to the ingestion service
  # (required)
  externalHostname: ""

  ingress:
    annotations: {}

    path: "/"

    pathType: "Prefix"

    tlsSecret: ""

  # (min: 1)
  replicas: 1

  internalEnabled: true

  otelEnabled: true

  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "768Mi"
      cpu: "400m"

  # Adjust the ingestion workload's logging level. Acceptable values are: OFF, ERROR, WARN, INFO,
  # DEBUG, TRACE, ALL.
  logLevel: "INFO"

  # Custom annotations for the Kubernetes service
  serviceAnnotations: {}

  # Kubernetes service type
  serviceType: "ClusterIP"

  # OP5 Ingestion workload
  op5:
    enabled: false

    # (min: 1)
    replicas: 1

    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "768Mi"
        cpu: "400m"

    # Custom annotations for the Kubernetes service
    serviceAnnotations: {}

    # (min: 1, max: 65535)
    servicePort: 7201

    # Kubernetes service type
    serviceType: "LoadBalancer"

# Sink Daemon workload
sinkd:
  # (min: 100000, max: 10000000)
  entityCacheMaxSize: 250000

  image: "obcerv/sinkd"

  jvmOpts: "-Xms768M -Xmx768M"

  nodeSelector: {}

  rawJvmOpts: "-Xms768M -Xmx768M"

  rawNodeSelector: {}

  rawReplicas: 1

  # By default, Sinkd for raw metrics requires an estimated 196M of non-heap memory. Set memory
  # request and limits based on the total memory formula: 196M + heap_memory_required = total_memory.
  rawResources:
    requests:
      memory: "1Gi"
      cpu: "250m"
    limits:
      memory: "1Gi"
      cpu: "500m"

  # (min: 1)
  replicas: 1

  # By default, Sinkd requires an estimated 184M of non-heap memory. Set memory request and limits
  # based on the total memory formula: 184M + heap_memory_required = total_memory.
  resources:
    requests:
      memory: "1Gi"
      cpu: "250m"
    limits:
      memory: "1Gi"
      cpu: "500m"

  # (min: 250000, max: 15000000)
  timeseriesCacheMaxSize: 500000

  # Metrics sink
  metrics:
    # (min: 1)
    maxPollRecords: 5000

  # Loki sink
  loki:
    # (min: 1)
    maxPollRecords: 2000

  # Adjust the sinkd workload's logging level. Acceptable values are: OFF, ERROR, WARN, INFO, DEBUG,
  # TRACE, ALL.
  logLevel: "INFO"

# Identity and Access Management
iam:
  ingress:
    annotations: {}

    path: "/auth"

    pathType: "Prefix"

    tlsSecret: ""

  toolsImage: "obcerv/iam-tools"

  # Keycloak workload
  keycloak:
    image: "obcerv/keycloak"

    resources:
      requests:
        memory: "320Mi"
        cpu: "100m"
      limits:
        memory: "768Mi"
        cpu: "400m"

    # Custom annotations for the Kubernetes service
    serviceAnnotations: {}

    # Kubernetes service type
    serviceType: "ClusterIP"

    # Adjust the keycloak workload's logging level. Acceptable values are: OFF, FATAL, ERROR, WARN,
    # INFO, DEBUG, TRACE, ALL.
    logLevel: "INFO"

# Platform Daemon workload
platformd:
  # (min: 0)
  entityQueryFetchSize: 10000

  entityUtilityImage: "obcerv/entity-containment-rules"

  image: "obcerv/platformd"

  # Adjust the platformd workload's logging level. Acceptable values are: OFF, ERROR, WARN, INFO,
  # DEBUG, TRACE and ALL.
  logLevel: "INFO"

  # (min: 0)
  metricQueryFetchSize: 10000

  nodeSelector: {}

  # (min: 1)
  replicas: 1

  jvmOpts: "-Xms256M -Xmx512M"

  # By default, Platformd requires an estimated 716M of non-heap memory. Set memory request and limits
  # based on the total memory formula: 716M + heap_memory_required = total_memory.
  resources:
    requests:
      memory: "1024Mi"
      cpu: "300m"
    limits:
      memory: "1280Mi"
      cpu: "500m"

# Data Pipeline Daemon workload
dpd:
  image: "obcerv/dpd"

  # Adjust the dpd workload's logging level. Acceptable values are: OFF, ERROR, WARN, INFO, DEBUG,
  # TRACE and ALL.
  logLevel: "INFO"

  nodeSelector: {}

  # (min: 1)
  replicas: 1

  jvmOpts: "-Xms256M -Xmx1024M"

  # The allowed wait period for data migration to occur in Hazelcast during process shutdown
  # (min: 1)
  shutdownGracePeriodSeconds: 300

  # The size in MB of the LRU cache used by RocksDB for entities
  # (min: 0)
  entitiesInMemoryCacheSizeMb: 100

  # Max poll records for Kafka consumers
  # (min: 1)
  kafkaConsumerMaxPollRecords: 5000

  metricsMultiplexer:
    # Max cache size for filter predicate result caching of metrics multiplexer
    # (min: 1)
    maxFilterResultCacheSize: 50000

    # Max concurrent operations to test filter predicates in the metrics multiplexer pipeline
    # (min: 1)
    maxConcurrentOps: 100

    # The local parallelism of the metrics multiplexer job per instance
    # (min: 1)
    localParallelism: 2

  # Data Pipeline Daemon resources
  resources:
    requests:
      memory: "1536Mi"
      cpu: "300m"
    limits:
      memory: "2Gi"
      cpu: "2"

  # Thresholds for self-monitoring tasks
  selfMonitoringThresholds:
    default_aggregation_window_seconds: 900
    default_hysteresis_seconds: 1800
    entities_partition_lag_warn: 2000
    entities_partition_lag_critical: 10000
    events_partition_lag_warn: 2000
    events_partition_lag_critical: 5000
    logs_partition_lag_warn: 10000
    logs_partition_lag_critical: 25000
    metrics_partition_lag_warn: 10000
    metrics_partition_lag_critical: 50000
    late_metrics_ignored_warn: 50000
    late_metrics_ignored_critical: 150000

# Key/Value Store workload
kvStore:
  # The number of historical entries to retain for key-values for which version history is needed.
  # (min: 10, max: 100)
  versionHistoryLimit: 20

  # Adjust the key/value store workload's logging level. Acceptable values are: OFF, ERROR, WARN,
  # INFO, DEBUG, TRACE and ALL.
  logLevel: "INFO"

  # (min: 1)
  replicas: 1

  resources:
    requests:
      memory: "384Mi"
      cpu: "300m"
    limits:
      memory: "768Mi"
      cpu: "300m"

# License Daemon Workload
licenced:
  image: "obcerv/licenced"

  # Adjust the license daemon workload's logging level. Acceptable values are: OFF, ERROR, WARN, INFO,
  # DEBUG, TRACE and ALL.
  logLevel: "INFO"

  resources:
    requests:
      memory: "384Mi"
      cpu: "300m"
    limits:
      memory: "768Mi"
      cpu: "300m"

# Metric Forecastd Daemon workload
metricForecastd:
  image: "obcerv/metrics-forecastd"

  # (min: 1)
  replicas: 1

  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "768Mi"
      cpu: "300m"

# Logs and Metrics Collection
collection:
  # Dimensions to globally ignore
  dimensionExcludes: []

  # Collection Agent image
  image: "obcerv/collection-agent"

  # Self-metrics Collection
  metrics:
    # Restrict collection to specific namespaces. If empty, only the Obcerv installation namespace
    # will be collected. To collect all, specify "*".
    namespaces: []

    jvmOpts: "-Xms512M -Xmx512M"

    statsdPort: 8125

    statsdProtocol: "udp"

    resources:
      requests:
        memory: "768Mi"
        cpu: "500m"
      limits:
        memory: "768Mi"
        cpu: "500m"

    # AWS Metrics Collection
    aws:
      enabled: false

      secret: "aws-metrics"

      region: ""

      # (min: 300000)
      intervalMillis: 300000

    # Adjust the self-metrics collection logging level. Acceptable values are: OFF, ERROR, WARN, INFO,
    # DEBUG, TRACE, ALL.
    logLevel: "INFO"

  # Kubernetes Pod Log Collection
  logs:
    enabled: true

    # Restrict collection to specific namespaces. If empty, only the Obcerv installation namespace
    # will be collected. To collect all, specify "*".
    namespaces: []

  # Collection Agent DaemonSet
  daemonSet:
    loggers: []

    nodeSelector: {}

    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "1Gi"
        cpu: "400m"

    tolerations: []

    # Adjust the collection agent's DaemonSet logging level. Acceptable values are: OFF, ERROR, WARN,
    # INFO, DEBUG, TRACE, ALL.
    logLevel: "INFO"

# Entity evictions cron job
entityEvictions:
  image: "obcerv/entity-evictions"

  resources:
    requests:
      memory: "32Mi"
      cpu: "10m"
    limits:
      memory: "64Mi"
      cpu: "50m"

  cron: "0 * * * *"

  # Eviction rules.  There must be at least one rule defined.  Rules have an optional attribute
  # definition and a required expiration, expressed as an ISO-8601 duration. An attribute is used to
  # restrict a rule to entities where the attribute is present.  The default rules apply an expiration
  # of 2 hours to entities having an "ephemeral" attribute defined, and an expiration of 60 days to
  # all other entities.  A given entity's expiration is calculated using its `inactive-since`
  # attribute. For example, in order for a  non-ephemeral entity to be evicted, the difference between
  # the time given by its `inactive-since` attribute and the current time must be greater than 60
  # days.
  evictionRules:
  - attribute:
      name: "ephemeral"
    expiration: "PT2H"
  - expiration: "P60D"

